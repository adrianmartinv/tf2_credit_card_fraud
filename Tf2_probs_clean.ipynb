{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.python.ops import control_flow_util\n",
    "import tensorflow.keras as keras\n",
    "K = keras.backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tf2 config\n",
    "control_flow_util.ENABLE_CONTROL_FLOW_V2 = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#read the csv\n",
    "df = pd.read_csv(\"../Datasets/creditcard.csv\")\n",
    "\n",
    "#split data/labels\n",
    "X = df.iloc[:,1:-1]\n",
    "y = df.iloc[:,-1]\n",
    "del df\n",
    "\n",
    "\n",
    "#split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42)\n",
    "del X\n",
    "del y\n",
    "\n",
    "#scale the data\n",
    "sc = StandardScaler()\n",
    "X_train.iloc[:,:] = sc.fit_transform(X_train)\n",
    "X_test.iloc[:,:] = sc.transform (X_test)\n",
    "\n",
    "#data without anomalies\n",
    "healthy_data = X_train[y_train==0]\n",
    "\n",
    "#indexes for later use\n",
    "X_train_indexes = tf.Variable(np.array(X_train.index), dtype=tf.float64)\n",
    "X_test_indexes = tf.Variable(np.array(X_test.index), dtype=tf.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def estimate_health(healthy_data):\n",
    "    \"\"\"\n",
    "        returns the mean and variance of the healthy data\n",
    "    \"\"\"\n",
    "    mean = K.mean( healthy_data, axis=0)\n",
    "    var = K.var( healthy_data, axis=0)\n",
    "    return mean, var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#means and variances of the healthy data\n",
    "mus_healthy, sigmasqs_healthy = estimate_health(healthy_data.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def probs(mus_healthy, sigmasqs_healthy, sample):\n",
    "    \"\"\"\n",
    "        returns the pdf of the sample given mus_healthy and sigmasqs_healthy\n",
    "    \"\"\"\n",
    "    norm = K.prod(tfp.distributions.Normal(mus_healthy, sigmasqs_healthy).prob(sample),axis=1)\n",
    "    multiv = tfp.distributions.MultivariateNormalDiag(mus_healthy, sigmasqs_healthy).prob(sample)\n",
    "\n",
    "    return norm, multiv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pdf of X_train of beign part of a healthy distribution\n",
    "norm_train, multiv_train = probs(mus_healthy,sigmasqs_healthy, X_train.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def reduce_samples(sample, labels ,indexes, norm, multiv, percentile):\n",
    "    \"\"\"\n",
    "        returns the indexes of the min values for pdf of normal and multivariate distributions\n",
    "    \"\"\"\n",
    "\n",
    "    sample_probs_n = tf.stack((tf.cast(indexes, tf.float64), norm), axis=1)\n",
    "    sample_probs_norm = tf.gather(sample_probs_n, tf.where(sample_probs_n[:,1] <= tfp.stats.percentile(norm, percentile)))\n",
    "    sample_probs_norm = tf.reshape(sample_probs_norm, [tf.shape(sample_probs_norm)[0], 2])\n",
    "\n",
    "    sample_probs_m = tf.stack((tf.cast(indexes, tf.float64), multiv), axis=1)\n",
    "    sample_probs_multiv = tf.gather(sample_probs_m, tf.where(sample_probs_m[:,1] <= tfp.stats.percentile(multiv, percentile)))\n",
    "    sample_probs_multiv = tf.reshape(sample_probs_multiv, [tf.shape(sample_probs_multiv)[0], 2])\n",
    "\n",
    "    a = sample_probs_multiv[:,0]\n",
    "    b = sample_probs_norm[:,0]\n",
    "\n",
    "    return a,b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#reduction of the samples\n",
    "\n",
    "a,b = reduce_samples(X_train.values, y_train.values, X_train_indexes, norm_train, multiv_train, 25)\n",
    "idx = np.intersect1d(a.numpy(), b.numpy())\n",
    "idx = np.array(idx, dtype=np.int32)\n",
    "idx = tf.Variable(idx, dtype=tf.int32)\n",
    "\n",
    "\n",
    "X_train_red_norm, X_train_red_multi = probs(mus_healthy, sigmasqs_healthy, tf.gather(X_train.values, idx))\n",
    "c, d = reduce_samples(tf.gather(X_train.values, idx), tf.gather(y_train.values, idx), idx, X_train_red_norm, X_train_red_multi, 25)\n",
    "idx2 = np.intersect1d(c.numpy(), d.numpy())\n",
    "idx2 = np.array(idx2, dtype=np.int32)\n",
    "idx2 = tf.Variable(idx2, dtype=tf.int32)\n",
    "\n",
    "\n",
    "X_train_red = tf.gather(X_train.values, idx2)\n",
    "y_train_red = tf.gather(y_train.values, idx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11927/11927 [==============================] - 3s 260us/sample - loss: 0.0049 - accuracy: 0.9947\n",
      "Epoch 2/10\n",
      "11927/11927 [==============================] - 4s 323us/sample - loss: 0.0043 - accuracy: 0.9956\n",
      "Epoch 3/10\n",
      "11927/11927 [==============================] - 2s 195us/sample - loss: 0.0042 - accuracy: 0.9956\n",
      "Epoch 4/10\n",
      "11927/11927 [==============================] - 2s 199us/sample - loss: 0.0026 - accuracy: 0.9971\n",
      "Epoch 5/10\n",
      "11927/11927 [==============================] - 2s 189us/sample - loss: 0.0013 - accuracy: 0.9987\n",
      "Epoch 6/10\n",
      "11927/11927 [==============================] - 3s 219us/sample - loss: 9.9581e-04 - accuracy: 0.9991\n",
      "Epoch 7/10\n",
      "11927/11927 [==============================] - 3s 276us/sample - loss: 9.4252e-04 - accuracy: 0.9991\n",
      "Epoch 8/10\n",
      "11927/11927 [==============================] - 2s 145us/sample - loss: 8.9773e-04 - accuracy: 0.9991\n",
      "Epoch 9/10\n",
      "11927/11927 [==============================] - 2s 194us/sample - loss: 8.5494e-04 - accuracy: 0.9992\n",
      "Epoch 10/10\n",
      "11927/11927 [==============================] - 3s 234us/sample - loss: 8.7598e-04 - accuracy: 0.9991\n"
     ]
    }
   ],
   "source": [
    "#Sequiential model\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr=0.2, momentum=.3, nesterov=True)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(100, activation='sigmoid', input_dim=X_train_red.shape[1]))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(40, activation='tanh'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(X_train_red, y_train_red, epochs=10, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "\n",
    "preds_train = model.predict_classes(X_train)\n",
    "preds_test = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some stadistics\n",
    "\n",
    "def confusionm(labels,pred):\n",
    "    matrix = confusion_matrix(labels,pred)\n",
    "    print(matrix)\n",
    "\n",
    "def classif_rep(labels,pred):\n",
    "    report = classification_report(labels,pred)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data\n",
      "[[190429     48]\n",
      " [    73    270]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    190477\n",
      "           1       0.85      0.79      0.82       343\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    190820\n",
      "   macro avg       0.92      0.89      0.91    190820\n",
      "weighted avg       1.00      1.00      1.00    190820\n",
      "\n",
      "test data\n",
      "[[93805    33]\n",
      " [   26   123]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     93838\n",
      "           1       0.79      0.83      0.81       149\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     93987\n",
      "   macro avg       0.89      0.91      0.90     93987\n",
      "weighted avg       1.00      1.00      1.00     93987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#results\n",
    "\n",
    "print(\"train data\")\n",
    "confusionm(y_train, preds_train)\n",
    "classif_rep(y_train, preds_train)\n",
    "\n",
    "print(\"test data\")\n",
    "confusionm(y_test, preds_test)\n",
    "classif_rep(y_test, preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
